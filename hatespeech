### data preprocessing ###

#file read
import pickle
from pprint import pprint
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from collections import Counter, defaultdict
from nltk.stem.porter import *
from sklearn.model_selection import KFold
import numpy as np
import os
os.chdir('Downloads')


def url_process(data, mode):
    '''
    https 주소 제거(0) or URLHERE 교체(1)
    '''
    giant_url_regex = ('http[s]?:?//(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+(#[0-9]*)?') # github 참조
    new_data = []
    for cat, doc in data:
        if mode == 1:
            new_data.append([cat, re.sub(giant_url_regex, 'URLHERE', doc)])
        elif mode == 0:
            new_data.append([cat, re.sub(giant_url_regex, '', doc)])
    return new_data

def mention_process(data, mode):
    '''
    mention 제거(0) or MENTIONHERE 교체(1)
    '''
    mention_regex = ('@[\w\-]+') # github 참조
    new_data = []
    for cat, doc in data:
        if mode == 1:
            new_data.append([cat, re.sub(mention_regex, 'MENTIONHERE', doc)])
        elif mode == 0:
            new_data.append([cat, re.sub(mention_regex, '', doc)])
    return new_data

def emoji_process(data, mode):
    '''
    emoji 처리 0: 지움 1: EMOJIHERE 2: emoji 사이 띄어쓰기
    '''
    emoji_regex = ('(&#[0-9]+;)')
    new_data = []
    for cat, doc in data:
        if mode == 1:
            new_data.append([cat, re.sub(emoji_regex, 'EMOJIHERE', doc)])
        elif mode == 0:
            new_data.append([cat, re.sub(emoji_regex, '', doc)])

    return new_data

def tokenize(sentence):
    return nltk.tokenize.word_tokenize(sentence)

def pos_tag(words):
    return [w1+'_'+w2 for w1,w2 in nltk.pos_tag(words)]






def data_process(filename):

    f=open(filename, 'r', encoding='utf-8')
    data = [line.split('\t') for line in f]
    f.close()
    for line in data:

        # 0: non 1: offensive 2: hate
        if line[0] == 'non-offensive':
            line[0] = 0
        elif line[0] == 'offensive' :
            line[0] = 1
        elif line[0] == 'hate' :
            line[0] = 2

        #line[1] = re.sub(' RT ','',line[1]) # RT 제거
        line[1] = re.sub('\.','',line[1]) # 마침표 제거
        line[1] = re.sub("``",'',line[1]) #``제거
        line[1] = re.sub("''",'',line[1]) #''제거
        #line[1] = re.sub('[)(-:]','',line[1])
        line[1] = re.sub('[,!?:]', '', line[1]) #,!?제거
        line[1] = re.sub('(&#[0-9]+;)', r' \1 ', line[1]) # emoji 띄어쓰기
        
        #line[1] = re.sub(' #([\w\-]+)', r'HASHTAGHERE \1', line[1]) #hashtags
        line[1] = re.sub(' #([\w\-]+)', r' \1', line[1]) #hashtags 지움


    
        ###아래 부분 주석처리 on/off 해가면서 data 처리종류 조절###


    data = [[cat, doc.lower()] for cat, doc in data] #소문자


    data = url_process(data, 0) #주소처리. 0 -> 주소지우기 1 -> URLHERE로 변경
    data = mention_process(data, 0) #멘션처리. 0 -> 지우기 1 -> MENTIONHERE로 변경
    data = emoji_process(data, 0) #멘션처리. 0 -> 지우기 1 -> EMOJIHERE로 변경
    
    stopwords = nltk.corpus.stopwords.words("english")

    for line in data:
        line[1] = " ".join(word for word in line[1].split() if word not in stopwords) #stopwords 제거
        line[1] = tokenize(line[1]) #tokenize
        line[1] = [PorterStemmer().stem(t) for t in line[1]] #stemmer -> 다만 urlher, mentioher, emojiher 로 바뀜
        #line[1] = pos_tag(line[1]) #pos tagging



    return data
  
f = open('bad_words.txt', 'r')
bad_words = []
bad_words_bi = []
for line in f:
    splited = line.split()
    if len(splited) == 1:
        bad_words.append(PorterStemmer().stem(splited[0]))
    elif len(splited) == 2:
        bad_words_bi.append((PorterStemmer().stem(splited[0]),PorterStemmer().stem(splited[1])))
f.close()


def cnt_vocab(doc, vocabulary):
    return sum([1 for word in doc if word in vocabulary])
	#return {word: word in doc for word in vocabulary}

def rt_count(doc):
    return sum([1 for word in doc if re.search('rt_?(\w)*', word)])

def url_count(doc):
    return sum([1 for word in doc if re.search('urlher_?(\w)*', word)])

def mention_count(doc):
    return sum([1 for word in doc if re.search('mentionher_?(\w)*', word)])
    
def emoji_count(doc):
    return sum([1 for word in doc if re.search('emojiher_?(\w)*', word)])

def hash_count(doc):
    return sum([1 for word in doc if re.search('hashtagher_?(\w)*', word)])

def length_sentence(doc):
    return len(doc)

def bi_count(doc, voca):
    return sum([1 for bigrm in list(nltk.bigrams(doc)) if bigrm in voca])

def get_all_features(doc, freq_voca, freq_voca_bi):

    
    return {
        'non_count':cnt_vocab(doc,freq_voca[0]) ,
        'ofns_count':cnt_vocab(doc,freq_voca[1]),'hate_count':cnt_vocab(doc,freq_voca[2]) ,'len':length_sentence(doc),
        'non_bi':bi_count(doc,freq_voca_bi[0]) , 'rt':rt_count(doc),
        #'emoji':emoji_count(doc),
        'bad_count': cnt_vocab(doc, bad_words), 'bad_bi': bi_count(doc,bad_words_bi),
        'ofns_bi':bi_count(doc,freq_voca_bi[1]),'hate_bi':bi_count(doc,freq_voca_bi[2])}
    
    '''
    return {'non_count':cnt_vocab(doc,freq_voca[0]) ,'ofns_count':cnt_vocab(doc,freq_voca[1]),'hate_count':cnt_vocab(doc,freq_voca[2]), 'rt':rt_count(doc), 
    'url':url_count(doc), 'emoji':emoji_count(doc) ,'len':length_sentence(doc), 'hastags': hash_count(doc),
    'non_bi':bi_count(doc,freq_voca_bi[0]) ,'ofns_bi':bi_count(doc,freq_voca_bi[1]),'hate_bi':bi_count(doc,freq_voca_bi[2])}
    '''


if __name__ == "__main__":
    
    ### preprocess 시간 오래 걸리므로 처음에 조건 바꿀때만 pkl로 저장, 이후는 주석처리 후 저장된 데이터로 훈련###
    
    data = data_process("train.txt")
    #f = open('preprocessed_0.pkl','wb')
    #pickle.dump(data, f)
    #print("preprocessed data saved.")
    #f.close()
    
    ####
    '''
    ### 저장된 preprocessed data 불러오기 ###

    f = open('preprocessed_0.pkl', 'rb')
    data = pickle.load(f)
    f.close()
    print("proprocessed data loaded")

    #####
    '''
    
    category = defaultdict(list)
    category[0] = 'non-offensive'
    category[1] = 'offensive'
    category[2] = 'hate'
    '''
    traint = []
    testt = []

    kfolded = KFold(5,False).split(data)
    for i, v in enumerate(kfolded):
      traint.append(v[0].tolist())
      testt.append(v[1].tolist())

    #print(train)
    #print(test)
    '''
    
    '''#################

    ##########################11111111111
    print('trial 1')


    boundary = int(len(data) * 0.2)
    test = data[:boundary]
    #dev = data[boundary:2*boundary]
    train = data[boundary:]

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])

    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    #clf_me.show_most_informative_features()
    
    
    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    accuracy1= nltk.classify.accuracy(clf_me, test_set)
    print('accuracy:',accuracy1)
    
    predicts = []
    refernc = []
    for cat, doc in test:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision1 = np.zeros(3)
    recall1 = np.zeros(3)
    f11 = np.zeros(3)

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision1[i] = cm[i,i]/ptot
      recall1[i] = cm[i,i]/rtot
      f11[i] = 2*recall1[i]*precision1[i]/(recall1[i]+precision1[i])
      print('precision of',category[i], ':', precision1[i])
      print('recall of',category[i], ':', recall1[i])
      print('f1 of',category[i], ':', f11[i])
      
    #print(cm.pretty_format(show_percents=True))

    
        ##########################2222222222
    print('trial 2')

    #train = traint[0]
    #test = testt[0]
    #boundary = int(len(data) * 0.2)
    test = data[boundary:2*boundary]
    train = data[:boundary] + data[2*boundary:]
    
    #print(data[:50])
    #print('data loaded')
    #vocabulary = {morph for cat, doc in train for morph in doc}
    #print('vocabular')

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])


    ####

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])


    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    clf_me.show_most_informative_features()
    
    
    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    accuracy2= nltk.classify.accuracy(clf_me, test_set)
    print('accuracy:',accuracy2)

    
    predicts = []
    refernc = []
    for cat, doc in test:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision2 = np.zeros(3)
    recall2 = np.zeros(3)
    f12 = np.zeros(3)

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision2[i] = cm[i,i]/ptot
      recall2[i] = cm[i,i]/rtot
      f12[i] = 2*recall2[i]*precision2[i]/(recall2[i]+precision2[i])
      print('precision of',category[i], ':', precision2[i])
      print('recall of',category[i], ':', recall2[i])
      print('f1 of',category[i], ':', f12[i])


    
        ##########################3333333
    print('trial 3')

    #train = traint[2]
    #test = testt[2]
    #boundary = int(len(data) * 0.2)
    #test = data[:boundary]
    #dev = data[boundary:2*boundary]
    #train = data[boundary:]
    
    test = data[2*boundary:3*boundary]
    train = data[:2*boundary] + data[3*boundary:]
    
    #print(data[:50])
    #print('data loaded')
    #vocabulary = {morph for cat, doc in train for morph in doc}
    #print('vocabular')

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])


    ####

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])


    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    clf_me.show_most_informative_features()
    
    
    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    accuracy3= nltk.classify.accuracy(clf_me, test_set)
    print('accuracy:',accuracy3)

    
    predicts = []
    refernc = []
    for cat, doc in test:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision3 = np.zeros(3)
    recall3 = np.zeros(3)
    f13 = np.zeros(3)
    

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision3[i] = cm[i,i]/ptot
      recall3[i] = cm[i,i]/rtot
      f13[i] = 2*recall3[i]*precision3[i]/(recall3[i]+precision3[i])
      print('precision of',category[i], ':', precision3[i])
      print('recall of',category[i], ':', recall3[i])
      print('f1 of',category[i], ':', f13[i])


#################444444


    
    print('trial 4')

    #train = traint[3]
    #test = testt[3]
    #boundary = int(len(data) * 0.2)
    #test = data[:boundary]
    #dev = data[boundary:2*boundary]
    #train = data[boundary:]
    
    test = data[3*boundary:4*boundary]
    train = data[:3*boundary] + data[4*boundary:]
    
    #print(data[:50])
    #print('data loaded')
    #vocabulary = {morph for cat, doc in train for morph in doc}
    #print('vocabular')

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])


    ####

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])


    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    clf_me.show_most_informative_features()
    
    
    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    accuracy4= nltk.classify.accuracy(clf_me, test_set)
    print('accuracy:',accuracy4)

    
    predicts = []
    refernc = []
    for cat, doc in test:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision4 = np.zeros(3)
    recall4 = np.zeros(3)
    f14 = np.zeros(3)

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision4[i] = cm[i,i]/ptot
      recall4[i] = cm[i,i]/rtot
      f14[i] = 2*recall4[i]*precision4[i]/(recall4[i]+precision4[i])
      print('precision of',category[i], ':', precision4[i])
      print('recall of',category[i], ':', recall4[i])
      print('f1 of',category[i], ':', f14[i])
      
 
    
        ##########################5555555
    print('trial 5')

    #train = traint[4]
    #test = testt[4]
    #boundary = int(len(data) * 0.2)
    #test = data[:boundary]
    #dev = data[boundary:2*boundary]
    #train = data[boundary:]
    
    test = data[4*boundary:]
    train = data[:4*boundary]
    
    #print(data[:50])
    #print('data loaded')
    #vocabulary = {morph for cat, doc in train for morph in doc}
    #print('vocabular')

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])


    ####

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])


    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    clf_me.show_most_informative_features()
    
    
    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    accuracy5= nltk.classify.accuracy(clf_me, test_set)
    print('accuracy:',accuracy5)

    predicts = []
    refernc = []
    for cat, doc in test:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision5 = np.zeros(3)
    recall5 = np.zeros(3)
    f15 = np.zeros(3)

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision5[i] = cm[i,i]/ptot
      recall5[i] = cm[i,i]/rtot
      f15[i] = 2*recall5[i]*precision5[i]/(recall5[i]+precision5[i])

      print('precision of',category[i], ':', precision5[i])
      print('recall of',category[i], ':', recall5[i])
      print('f1 of',category[i], ':', f15[i])

      
      
      
    
    
    ###total
    acc_total = (accuracy1+accuracy2+accuracy3+accuracy4+accuracy5)/5
    precision_total = np.zeros(3)
    recall_total = np.zeros(3)
    f1_total = np.zeros(3)
    print('total scores')
    print('accuracy total :', acc_total)
    for i in range(3):
      precision_total[i] = (precision1[i]+precision2[i]+precision3[i]+precision4[i]+precision5[i])/5
      recall_total[i] = (recall1[i]+recall2[i]+recall3[i]+recall4[i]+recall5[i])/5
      f1_total[i] = 2*recall_total[i]*precision_total[i]/(recall_total[i]+precision_total[i])

      print('precision of',category[i], ':', precision_total[i])
      print('recall of',category[i], ':', recall_total[i])
      print('f1 of',category[i], ':', f1_total[i])


    '''#################

      
    #f = open('errors.txt', 'w')
    '''
    for cat, doc in dev:
        predicted = clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi))
        if cat != predicted:
            print(cat, predicted, doc)
            #f.write(cat, predicted, doc)
    '''
    #f.close()

    ###181204 메모: 전처리 - 주소, 멘션, 이모티콘 지움/ 특성: non, off, hate 상위 10-20% 출현 단어 개수 / dev accuracy: 80%언저리
    ### rt, length, bigram 추가 83%
    ### emoji, mention, url 반영시 82.4%
    ### emoji, mention, url, rt 반영 안하고 pos tagging 안하면 84.7%
    ### 상위 100개 단어시 85%
    ### bad_words.txt 반영시 87%
    ### emoji 제거시 

    
    
    
    
    ##########################final
    print('final')

    train = data
    #test = testt[0]
    #boundary = int(len(data) * 0.2)
    #test = data[:boundary]
    #dev = data[boundary:2*boundary]
    #train = data[boundary:]
    
    #print(data[:50])
    #print('data loaded')
    #vocabulary = {morph for cat, doc in train for morph in doc}
    #print('vocabular')

    ### train 에서 각 범주별 높은 빈도로 출현하는 단어
    bigdoc = defaultdict(list)
    uselesswords = [';','rt','@','&',"'",'"','#',')','(','-',':','``',"''"]

    for line in train:
	    bigdoc[line[0]].extend(line[1])

    counts = defaultdict(Counter)
    counts = {1: Counter(bigdoc[1]), 0: Counter(bigdoc[0]), 2: Counter(bigdoc[2])}

    freq_voca = defaultdict(list)
    for i in range(0,3):
        #freq_voca[i] = [word for word, cnt in counts[i].most_common(int(0.15*len(counts[i])))]
        freq_voca[i] = [word for word, cnt in counts[i].most_common(70) if word not in uselesswords]
        print(freq_voca[i])


    ####

    ###bigram
    bigdoc_bi = defaultdict(list)

    for line in train:
	    bigdoc_bi[line[0]].extend(list(nltk.bigrams([word for word in line[1] if word not in uselesswords])))

    counts_bi = defaultdict(Counter)
    counts_bi = {1: Counter(bigdoc_bi[1]), 0: Counter(bigdoc_bi[0]), 2: Counter(bigdoc_bi[2])}

    freq_voca_bi = defaultdict(list)
    for i in range(0,3):
        #freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(int(0.15*len(counts_bi[i])))]
        freq_voca_bi[i] = [word for word, cnt in counts_bi[i].most_common(100)]
        print(freq_voca_bi[i])


    ######
    
    train_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in train]
    #test_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in test]
    #dev_set = [(get_all_features(doc,freq_voca, freq_voca_bi), cat) for cat, doc in dev]
    
    clf_me = nltk.MaxentClassifier.train(train_set, max_iter=100)
    #clf_me.show_most_informative_features()
    
    #f = open('preprocessed_0.pkl','wb')
    #pickle.dump(data, f)
    #print("preprocessed data saved.")
    #f.close()
    
    f= open('classifier_group8.pkl', 'wb')
    pickle.dump(clf_me, f)
    print('classifier saved')
    f.close()
    
    '''
    ###############################실제 실험집합 테스트##########################
    ################저장된 classifier 불러오기
    f= open('classifier_group8.pkl', 'wb')
    clf_me = pickle.load(f)
    print('classifier loaded')
    f.close()

    #test = data_process('파일이름')
    '''



    #print(nltk.classify.accuracy(clf_me, dev_set))
    #print(nltk.ConfusionMatrix().pretty_format(clf_me,dev_set))
    #accuracy2= nltk.classify.accuracy(clf_me, test_set)
    '''

    predicts = []
    refernc = []
    for cat, doc in dev:
      refernc.append(cat)
      predicts.append(clf_me.classify(get_all_features(doc,freq_voca,freq_voca_bi)))
     
    
    cm = nltk.ConfusionMatrix(refernc,predicts)
    
    precision2 = np.zeros(3)
    recall2 = np.zeros(3)
    f12 = np.zeros(3)

    for i in range(3):
      ptot = 0
      rtot = 0
      for j in range(3):
        ptot += cm[i,j]
        rtot += cm[j,i]
    
      precision2[i] = cm[i,i]/ptot
      recall2[i] = cm[i,i]/rtot
      f12[i] = 2*recall2[i]*precision2[i]/(recall2[i]+precision2[i])
      print('precision of',category[i], ':', precision2[i])
      print('recall of',category[i], ':', recall2[i])
      print('f1 of',category[i], ':', f12[i])

    '''

